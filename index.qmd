---
title: "Weekly Summary Week 13"
author: "Jack Benadon"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Jan 17

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::

Provide more concrete details here. You can also use footenotes[^footnote] if you like

```{R results='hide'}
#| output: false
library(dplyr) 
library(purrr) 
library(glmnet)
library(torch)
library(ISLR2)
library(tidyr) 
library(readr) 
library(caret)
library(mlbench)
library(nnet)
library(class)
library(rpart)
library(e1071) 
library(luz) 
library(torchvision)
library(dimRed)
library(RSpectra)
```

### PCA

##### Inforamtion in this context refers to variability in the data

__Step 1 of PCA__
The first principal component $Z_1$ is the normalized linear combination of the features

such that 
- $Z_1$ has the largest possible variance
- the sum of the squares is = 1 _(This is to put a threshold on $Z_1$)_

__Step 2__
the second principal component $Z_2$ is the normalized linear combination of the features:

such that
- $V_2$ is orthogonal to $V_1$
- $Z_2$ has the largest possible variance
- the sum of the squares is = 1 _(This is to put a threshold on $Z_1$)_

__Step q__
The qth principal component $Z_q$ is the normalized linear combination of the features:

such that:
- $Z-q$  has the largest possible variance
- $Z_q$ is othogonal to the span of $V_1$ through $V(q-1)$
- the sm of the squares is = 1

### Example in R
We can do PCA in R like this:
```{R}
data <- tibble(
  x1 = rnorm(100, mean = 0, sd = 1),
  x2 = x1 + rnorm(100, mean = 0, sd = 0.1)
)

pca <- princomp(data, cor = TRUE)
summary(pca)
```

```{R}
screeplot(pca, type="l")
```

How to incorporate PCA into our regression analysis:
We can use the function prcomp() to perform PCA.







## Thursday, Jan 19



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::

### NOnlinear dimension reduction
```{R}
generate_two_spirals <- function () {
  set.seed(42)
  n <- 500
  noise <- 500
  t <- (1:n) / n * 4 * pi
  x1 <- t * (sin(t) + rnorm(n,0,noise))
  x2 <- t*(cos(t) + rnorm(n,0,noise))
  

}  
gener
```

ISOmap stands for 
Isometric
manifold
approximated and projection

Theres t-SNE, UMAP, and MDS. They're useful for biostats and similar fields.

You can use isomap in the RDRToolbox package.

```{R}
library(RDRToolbox)
```

```{R}
isomap <- Isomap(df[,1:2] %>% as.matrix)
```

### Autoencoder
One of the most popular techniques for dimension reduction. It does this by constructing the following neural network architecture:

It takes a high dimensional dataset and then outputs it in 2 dimensions. It has an encoder which takes it from the original state to a compressed one and then the decoder sets it back to its original state.


[^footnote]: You can include some footnotes here